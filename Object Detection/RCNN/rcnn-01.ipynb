{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q --upgrade selectivesearch torch_snippets \nfrom torch_snippets import *\nfrom tqdm import tqdm\nimport selectivesearch\nfrom torchvision import transforms, models, datasets\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torchvision.ops import nms\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-06T21:46:08.072264Z","iopub.execute_input":"2022-09-06T21:46:08.072877Z","iopub.status.idle":"2022-09-06T21:46:18.330257Z","shell.execute_reply.started":"2022-09-06T21:46:08.072840Z","shell.execute_reply":"2022-09-06T21:46:18.328876Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"IMAGES_ROOT = '../input/open-images-bus-trucks/images/images'\ndf_raw = pd.read_csv('../input/open-images-bus-trucks/df.csv')\ndf_raw.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-06T21:47:36.396258Z","iopub.execute_input":"2022-09-06T21:47:36.396628Z","iopub.status.idle":"2022-09-06T21:47:36.484840Z","shell.execute_reply.started":"2022-09-06T21:47:36.396595Z","shell.execute_reply":"2022-09-06T21:47:36.483747Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# dataset class\nclass OpenImages(Dataset):\n  def __init__(self, df, image_folder=IMAGES_ROOT):\n    self.root = image_folder\n    self.df = df\n    self.unique_images = df['ImageID'].unique()\n  \n  def __len__(self):\n    return len(self.unique_images)\n\n  def __getitem__(self, idx):\n    image_id = self.unique_images[idx]\n    image_path = f'{self.root}/{image_id}.jpg'\n    image = cv2.imread(image_path, 1)[..., ::-1] # convert BGR to RGB\n    h, w, _ = image.shape\n    df = self.df.copy()\n    df = df[df['ImageID'] == image_id]\n    boxes = df[['XMin', 'YMin', 'XMax', 'YMax']].values\n    boxes = (boxes * np.array([w, h, w, h])).astype(np.uint16).tolist()\n    classes = df['LabelName'].values.tolist()\n    return image, boxes, classes, image_path","metadata":{"execution":{"iopub.status.busy":"2022-09-06T21:47:37.536572Z","iopub.execute_input":"2022-09-06T21:47:37.536997Z","iopub.status.idle":"2022-09-06T21:47:37.546750Z","shell.execute_reply.started":"2022-09-06T21:47:37.536962Z","shell.execute_reply":"2022-09-06T21:47:37.545495Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"ds = OpenImages(df=df_raw)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T21:47:38.228764Z","iopub.execute_input":"2022-09-06T21:47:38.230089Z","iopub.status.idle":"2022-09-06T21:47:38.238318Z","shell.execute_reply.started":"2022-09-06T21:47:38.230047Z","shell.execute_reply":"2022-09-06T21:47:38.237261Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"im, bbs, clss, _ = ds[23]\nshow(im, bbs=bbs, texts=clss, sz=10)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T21:47:52.077581Z","iopub.execute_input":"2022-09-06T21:47:52.077960Z","iopub.status.idle":"2022-09-06T21:47:52.381453Z","shell.execute_reply.started":"2022-09-06T21:47:52.077928Z","shell.execute_reply":"2022-09-06T21:47:52.380638Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def extract_candidates(img):\n  img_lbl, regions = selectivesearch.selective_search(img, scale=200, min_size=100)\n  img_area = np.prod(img.shape[:2])\n  candidates = []\n  for r in regions:\n    if r['rect'] in candidates: continue\n    if r['size'] < (0.05*img_area): continue # ignore candidates that are less than 5% of the image\n    if r['size'] > img_area: continue # only include candidates that are same size or less than the image\n    candidates.append(list(r['rect']))\n  return candidates\n\ndef extract_iou(boxA, boxB, epsilon=1e-5):\n  x1 = max(boxA[0], boxB[0])\n  y1 = max(boxA[1], boxB[1])\n  x2 = min(boxA[2], boxB[2])\n  y2 = min(boxA[3], boxB[3])\n  width = x2 - x1\n  height = y2 - y1\n  if width < 0 or height < 0:\n    return 0.0\n  area_overlap = width * height\n  area_a = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n  area_b = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n  area_union = area_a + area_b - area_overlap\n  iou = area_overlap / area_union\n  return iou","metadata":{"execution":{"iopub.status.busy":"2022-09-06T21:48:00.866479Z","iopub.execute_input":"2022-09-06T21:48:00.866872Z","iopub.status.idle":"2022-09-06T21:48:00.876852Z","shell.execute_reply.started":"2022-09-06T21:48:00.866837Z","shell.execute_reply":"2022-09-06T21:48:00.875684Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"FPATHS, GTBBS, CLSS, DELTAS, ROIS, IOUS = [], [], [], [], [], []\nN = 500\nfor ix, (im, bbs, labels, fpath) in tqdm(enumerate(ds), total=N):\n    if(ix==N):\n        break\n    H, W, _ = im.shape\n    candidates = extract_candidates(im)\n    candidates = np.array([(x,y,x+w,y+h) for x,y,w,h in candidates])\n    ious, rois, clss, deltas = [], [], [], []\n    ious = np.array([[extract_iou(candidate, _bb_) for candidate in candidates] for _bb_ in bbs]).T\n    for jx, candidate in enumerate(candidates):\n        cx,cy,cX,cY = candidate\n        candidate_ious = ious[jx]\n        best_iou_at = np.argmax(candidate_ious)\n        best_iou = candidate_ious[best_iou_at]\n        best_bb = _x,_y,_X,_Y = bbs[best_iou_at]\n        if best_iou > 0.3: \n          clss.append(labels[best_iou_at])\n        else : \n          clss.append('background')\n        delta = np.array([_x-cx, _y-cy, _X-cX, _Y-cY]) / np.array([W,H,W,H])\n        deltas.append(delta)\n        rois.append(candidate / np.array([W,H,W,H]))\n    FPATHS.append(fpath)\n    IOUS.append(ious)\n    ROIS.append(rois)\n    CLSS.append(clss)\n    DELTAS.append(deltas)\n    GTBBS.append(bbs)\n\nFPATHS = [f'{IMAGES_ROOT}/{stem(f)}.jpg' for f in FPATHS] \nFPATHS, GTBBS, CLSS, DELTAS, ROIS = [item for item in [FPATHS, GTBBS, CLSS, DELTAS, ROIS]]","metadata":{"execution":{"iopub.status.busy":"2022-09-06T21:48:01.302650Z","iopub.execute_input":"2022-09-06T21:48:01.303314Z","iopub.status.idle":"2022-09-06T21:50:42.111850Z","shell.execute_reply.started":"2022-09-06T21:48:01.303281Z","shell.execute_reply":"2022-09-06T21:50:42.110143Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"targets = pd.DataFrame(flatten(CLSS), columns=['label'])\nlabel2target = {l:t for t,l in enumerate(targets['label'].unique())}\ntarget2label = {t:l for l,t in label2target.items()}\nbackground_class = label2target['background']","metadata":{"execution":{"iopub.status.busy":"2022-09-06T21:50:42.113852Z","iopub.execute_input":"2022-09-06T21:50:42.114207Z","iopub.status.idle":"2022-09-06T21:50:42.123503Z","shell.execute_reply.started":"2022-09-06T21:50:42.114170Z","shell.execute_reply":"2022-09-06T21:50:42.122434Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"label2target","metadata":{"execution":{"iopub.status.busy":"2022-09-06T21:50:42.125189Z","iopub.execute_input":"2022-09-06T21:50:42.125602Z","iopub.status.idle":"2022-09-06T21:50:42.138183Z","shell.execute_reply.started":"2022-09-06T21:50:42.125552Z","shell.execute_reply":"2022-09-06T21:50:42.136878Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n\ndef preprocess_image(img):\n  img = torch.tensor(img).permute(2, 0, 1)\n  img = normalize(img)\n  return img.to(device).float()\n\ndef decode(_y):\n  _, preds = _y.max(-1)\n  return preds","metadata":{"execution":{"iopub.status.busy":"2022-09-06T21:50:42.140908Z","iopub.execute_input":"2022-09-06T21:50:42.141165Z","iopub.status.idle":"2022-09-06T21:50:42.149112Z","shell.execute_reply.started":"2022-09-06T21:50:42.141142Z","shell.execute_reply":"2022-09-06T21:50:42.147767Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"class RCNNDataset(Dataset):\n  def __init__(self, fpaths, rois, labels, deltas, gtbbs):\n    self.fpaths = fpaths\n    self.gtbbs = gtbbs\n    self.rois = rois\n    self.labels = labels\n    self.deltas = deltas\n\n  def __len__(self): \n    return len(self.fpaths)\n\n  def __getitem__(self, idx):\n    fpaths = str(self.fpaths[idx])\n    image = cv2.imread(fpath, 1)[..., ::-1]\n    H, W, _ = image.shape\n    sh = np.array([W, H, W, H])\n    gtbbs = self.gtbbs[idx]\n    rois = self.rois[idx]\n    bbs = (np.array(rois)*sh).astype(np.uint16)\n    labels = self.labels[idx]\n    deltas = self.deltas[idx]\n    crops = [image[y:Y, x:X] for (x, y, X, Y) in bbs]\n    return image, crops, bbs, labels, deltas, gtbbs, fpath\n\n  def collate_fn(self, batch):\n    input, rois, rixs, labels, deltas = [], [], [], [], []\n    for idx in range(len(batch)):\n      image, crops, image_bbs, image_labels, image_deltas, image_gt_bbs, image_fpath = batch[idx]\n      crops = [cv2.resize(crop, (224, 224)) for crop in crops] # resize all proposals \n      crops = [preprocess_image(crop/255.0)[None] for crop in crops] \n      input.extend(crops)\n      labels.extend(label2target[c] for c in image_labels)\n      deltas.extend(image_deltas)\n\n    input = torch.cat(input).to(device)\n    labels = torch.Tensor(labels).long().to(device)\n    deltas = torch.Tensor(deltas).float().to(device)\n    return input, labels, deltas","metadata":{"execution":{"iopub.status.busy":"2022-09-06T21:50:42.151002Z","iopub.execute_input":"2022-09-06T21:50:42.152500Z","iopub.status.idle":"2022-09-06T21:50:42.167228Z","shell.execute_reply.started":"2022-09-06T21:50:42.152471Z","shell.execute_reply":"2022-09-06T21:50:42.165943Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2022-09-06T22:25:38.331350Z","iopub.execute_input":"2022-09-06T22:25:38.331752Z","iopub.status.idle":"2022-09-06T22:25:39.472380Z","shell.execute_reply.started":"2022-09-06T22:25:38.331724Z","shell.execute_reply":"2022-09-06T22:25:39.471040Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"n_train = 9*len(FPATHS)//10 # 450\ntrain_ds = RCNNDataset(FPATHS[:n_train], ROIS[:n_train], CLSS[:n_train], DELTAS[:n_train], GTBBS[:n_train])\ntest_ds = RCNNDataset(FPATHS[n_train:], ROIS[n_train:], CLSS[n_train:], DELTAS[n_train:], GTBBS[n_train:])\n\ntrain_loader = DataLoader(train_ds, batch_size=2, collate_fn=train_ds.collate_fn, drop_last=True)\ntest_loader = DataLoader(test_ds, batch_size=2, collate_fn=test_ds.collate_fn, drop_last=True)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T21:50:42.169411Z","iopub.execute_input":"2022-09-06T21:50:42.170982Z","iopub.status.idle":"2022-09-06T21:50:42.182255Z","shell.execute_reply.started":"2022-09-06T21:50:42.170953Z","shell.execute_reply":"2022-09-06T21:50:42.181007Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"vgg_backbone = models.vgg16(pretrained=True)\nvgg_backbone.classifier = nn.Sequential()\nfor param in vgg_backbone.parameters():\n  param.require_grad = False\nvgg_backbone.eval().to(device)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T21:51:08.104700Z","iopub.execute_input":"2022-09-06T21:51:08.105632Z","iopub.status.idle":"2022-09-06T21:51:51.292690Z","shell.execute_reply.started":"2022-09-06T21:51:08.105583Z","shell.execute_reply":"2022-09-06T21:51:51.291573Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"class RCNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        feature_dim = 25088\n        self.backbone = vgg_backbone\n        self.cls_score = nn.Linear(feature_dim, len(label2target))\n        self.bbox = nn.Sequential(\n              nn.Linear(feature_dim, 512),\n              nn.ReLU(),\n              nn.Linear(512, 4),\n              nn.Tanh(),\n            )\n        self.cel = nn.CrossEntropyLoss()\n        self.sl1 = nn.L1Loss()\n    def forward(self, input):\n        feat = self.backbone(input)\n        cls_score = self.cls_score(feat)\n        bbox = self.bbox(feat)\n        return cls_score, bbox\n    def calc_loss(self, probs, _deltas, labels, deltas):\n        detection_loss = self.cel(probs, labels)\n        ixs, = torch.where(labels != 0)\n        _deltas = _deltas[ixs]\n        deltas = deltas[ixs]\n        self.lmb = 10.0\n        if len(ixs) > 0:\n            regression_loss = self.sl1(_deltas, deltas)\n            return detection_loss + self.lmb * regression_loss, detection_loss.detach(), regression_loss.detach()\n        else:\n            regression_loss = 0\n            return detection_loss + self.lmb * regression_loss, detection_loss.detach(), regression_loss","metadata":{"execution":{"iopub.status.busy":"2022-09-06T21:51:51.294853Z","iopub.execute_input":"2022-09-06T21:51:51.295245Z","iopub.status.idle":"2022-09-06T21:51:51.305805Z","shell.execute_reply.started":"2022-09-06T21:51:51.295207Z","shell.execute_reply":"2022-09-06T21:51:51.304691Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def train_batch(inputs, model, optimizer, criterion):\n    input, clss, deltas = inputs\n    model.train()\n    optimizer.zero_grad()\n    _clss, _deltas = model(input)\n    loss, loc_loss, regr_loss = criterion(_clss, _deltas, clss, deltas)\n    accs = clss == decode(_clss)\n    loss.backward()\n    optimizer.step()\n    return loss.detach(), loc_loss, regr_loss, accs.cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2022-09-06T21:51:55.355400Z","iopub.execute_input":"2022-09-06T21:51:55.355880Z","iopub.status.idle":"2022-09-06T21:51:55.364108Z","shell.execute_reply.started":"2022-09-06T21:51:55.355840Z","shell.execute_reply":"2022-09-06T21:51:55.363052Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef validate_batch(inputs, model, criterion):\n    input, clss, deltas = inputs\n    with torch.no_grad():\n        model.eval()\n        _clss,_deltas = model(input)\n        loss, loc_loss, regr_loss = criterion(_clss, _deltas, clss, deltas)\n        _, _clss = _clss.max(-1)\n        accs = clss == _clss\n    return _clss, _deltas, loss.detach(), loc_loss, regr_loss, accs.cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2022-09-06T21:52:02.402540Z","iopub.execute_input":"2022-09-06T21:52:02.402929Z","iopub.status.idle":"2022-09-06T21:52:02.409488Z","shell.execute_reply.started":"2022-09-06T21:52:02.402896Z","shell.execute_reply":"2022-09-06T21:52:02.408372Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"rcnn = RCNN().to(device)\ncriterion = rcnn.calc_loss\noptimizer = optim.SGD(rcnn.parameters(), lr=1e-3)\nn_epochs = 5\nlog = Report(n_epochs)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T22:25:52.129192Z","iopub.execute_input":"2022-09-06T22:25:52.130412Z","iopub.status.idle":"2022-09-06T22:25:52.263519Z","shell.execute_reply.started":"2022-09-06T22:25:52.130348Z","shell.execute_reply":"2022-09-06T22:25:52.262446Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"for epoch in range(n_epochs):\n\n    _n = len(train_loader)\n    for ix, inputs in enumerate(train_loader):\n        loss, loc_loss, regr_loss, accs = train_batch(inputs, rcnn, \n                                                      optimizer, criterion)\n        pos = (epoch + (ix+1)/_n)\n        log.record(pos, trn_loss=loss.item(), trn_loc_loss=loc_loss, \n                   trn_regr_loss=regr_loss, \n                   trn_acc=accs.mean(), end='\\r')\n        \n    _n = len(test_loader)\n    for ix,inputs in enumerate(test_loader):\n        _clss, _deltas, loss, \\\n        loc_loss, regr_loss, accs = validate_batch(inputs, \n                                                rcnn, criterion)\n        pos = (epoch + (ix+1)/_n)\n        log.record(pos, val_loss=loss.item(), val_loc_loss=loc_loss, \n                val_regr_loss=regr_loss, \n                val_acc=accs.mean(), end='\\r')\n\n# Plotting training and validation metrics\nlog.plot_epochs('trn_loss,val_loss'.split(','))","metadata":{"execution":{"iopub.status.busy":"2022-09-06T22:25:53.123651Z","iopub.execute_input":"2022-09-06T22:25:53.124679Z","iopub.status.idle":"2022-09-06T22:40:28.531711Z","shell.execute_reply.started":"2022-09-06T22:25:53.124644Z","shell.execute_reply":"2022-09-06T22:40:28.530746Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"def test_predictions(filename, show_output=True):\n    img = np.array(cv2.imread(filename, 1)[...,::-1])\n    candidates = extract_candidates(img)\n    candidates = [(x,y,x+w,y+h) for x,y,w,h in candidates]\n    input = []\n    for candidate in candidates:\n        x,y,X,Y = candidate\n        crop = cv2.resize(img[y:Y,x:X], (224,224))\n        input.append(preprocess_image(crop/255.)[None])\n    input = torch.cat(input).to(device)\n    with torch.no_grad():\n        rcnn.eval()\n        probs, deltas = rcnn(input)\n        probs = torch.nn.functional.softmax(probs, -1)\n        confs, clss = torch.max(probs, -1)\n    candidates = np.array(candidates)\n    confs, clss, probs, deltas = [tensor.detach().cpu().numpy() for tensor in [confs, clss, probs, deltas]]\n\n    ixs = clss!=background_class\n    confs, clss, probs, deltas, candidates = [tensor[ixs] for tensor in [confs, clss, probs, deltas, candidates]]\n    bbs = (candidates + deltas).astype(np.uint16)\n    ixs = nms(torch.tensor(bbs.astype(np.float32)), torch.tensor(confs), 0.05)\n    confs, clss, probs, deltas, candidates, bbs = [tensor[ixs] for tensor in [confs, clss, probs, deltas, candidates, bbs]]\n    if len(ixs) == 1:\n        confs, clss, probs, deltas, candidates, bbs = [tensor[None] for tensor in [confs, clss, probs, deltas, candidates, bbs]]\n    if len(confs) == 0 and not show_output:\n        return (0,0,224,224), 'background', 0\n    if len(confs) > 0:\n        best_pred = np.argmax(confs)\n        best_conf = np.max(confs)\n        best_bb = bbs[best_pred]\n        x,y,X,Y = best_bb\n    _, ax = plt.subplots(1, 2, figsize=(20,10))\n    show(img, ax=ax[0])\n    ax[0].grid(False)\n    ax[0].set_title('Original image')\n    if len(confs) == 0:\n        ax[1].imshow(img)\n        ax[1].set_title('No objects')\n        plt.show()\n        return\n    ax[1].set_title(target2label[clss[best_pred]])\n    show(img, bbs=bbs.tolist(), texts=[target2label[c] for c in clss.tolist()], ax=ax[1], title='predicted bounding box and class')\n    plt.show()\n    return (x,y,X,Y),target2label[clss[best_pred]],best_conf","metadata":{"execution":{"iopub.status.busy":"2022-09-06T22:40:28.533768Z","iopub.execute_input":"2022-09-06T22:40:28.534596Z","iopub.status.idle":"2022-09-06T22:40:28.551441Z","shell.execute_reply.started":"2022-09-06T22:40:28.534557Z","shell.execute_reply":"2022-09-06T22:40:28.550086Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"image, crops, bbs, labels, deltas, gtbbs, fpath = test_ds[10]\ntest_predictions(fpath)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T22:41:41.196860Z","iopub.execute_input":"2022-09-06T22:41:41.197225Z","iopub.status.idle":"2022-09-06T22:41:42.143837Z","shell.execute_reply.started":"2022-09-06T22:41:41.197186Z","shell.execute_reply":"2022-09-06T22:41:42.142932Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}